# Visi√≥n general -- Plataforma de Datos End-to-End

Este tutorial pr√°ctico te gu√≠a en la construcci√≥n de un **pipeline completo de datos de NYC Taxi** desde cero usando Bruin, una herramienta CLI unificada para ingesti√≥n, transformaci√≥n y calidad de datos.

Aprender√°s a construir un pipeline ELT listo para producci√≥n que:

* **Ingesta** datos reales de viajes de NYC Taxi desde APIs p√∫blicas usando Python
* **Transforma** y limpia datos crudos con SQL, aplicando estrategias incrementales y deduplicaci√≥n
* **Reporta** anal√≠tica agregada con controles de calidad integrados
* **Despliega** en infraestructura cloud (BigQuery)

Esta es una experiencia de aprendizaje pr√°ctica con asistencia de IA disponible mediante Bruin MCP. Sigue la secci√≥n del tutorial paso a paso a continuaci√≥n.

## üéØ Objetivos

* Entender c√≥mo se estructuran los proyectos de Bruin (`pipeline/pipeline.yml` + `pipeline/assets/`)
* Usar **estrategias de materializaci√≥n** de forma intencional (`append`, `time_interval`, etc.)
* Declarar **dependencias** y explorar el linaje (`bruin lineage`)
* Aplicar **metadatos** y **controles de calidad**
* Parametrizar ejecuciones con **variables del pipeline**

# Visi√≥n General - Plataforma de Datos de Extremo a Extremo

Este tutorial pr√°ctico te gu√≠a en la construcci√≥n de un **pipeline de datos completo con los taxis de Nueva York** desde cero usando Bruin‚Äîuna herramienta CLI unificada para ingesti√≥n, transformaci√≥n y calidad de datos.

Aprender√°s a construir un pipeline ELT listo para producci√≥n que:
- **Ingiere** datos reales de viajes en taxi de Nueva York desde APIs p√∫blicas usando Python
- **Transforma** y limpia datos en bruto con SQL, aplicando estrategias incrementales y deduplicaci√≥n
- **Reporta** anal√≠ticas agregadas con comprobaciones de calidad integradas
- **Despliega** en infraestructura en la nube (BigQuery)

Esta es una experiencia de aprendizaje pr√°ctico con asistencia de IA disponible a trav√©s de Bruin MCP. Sigue el tutorial detallado paso a paso que encontrar√°s m√°s abajo.

## Objetivos de Aprendizaje

- Entender c√≥mo se estructuran los proyectos Bruin (`pipeline/pipeline.yml` + `pipeline/assets/`)
- Usar **estrategias de materializaci√≥n** de forma intencionada (append, time_interval, etc.)
- Declarar **dependencias** y explorar el linaje (`bruin lineage`)
- Aplicar **metadatos** (columnas, claves primarias, descripciones) y **comprobaciones de calidad**
- Parametrizar ejecuciones con **variables de pipeline**

## Esquema del Tutorial

- **Parte 1**: ¬øQu√© es una Plataforma de Datos? - Aprende sobre los componentes del stack de datos moderno y d√≥nde encaja Bruin
- **Parte 2**: Configuraci√≥n de tu Primer Proyecto Bruin - Instala Bruin, inicializa un proyecto y configura entornos
- **Parte 3**: Pipeline ELT de Extremo a Extremo con Taxis de Nueva York - Construye las capas de ingesti√≥n, staging y reporting con datos reales
- **Parte 4**: Ingenier√≠a de Datos con Agente de IA - Usa Bruin MCP para construir pipelines con asistencia de IA
- **Parte 5**: Despliegue en BigQuery - Despliega tu pipeline local en Google BigQuery

## Estructura del Pipeline

La estructura sugerida separa la ingesti√≥n, el staging y el reporting, pero puedes organizar tu pipeline como prefieras.

Las partes obligatorias de un proyecto Bruin son:
- `.bruin.yml` en el directorio ra√≠z
- `pipeline.yml` en el directorio `pipeline/` (o en el directorio ra√≠z si mantienes todo en un √∫nico nivel)
- La carpeta `assets/` junto a `pipeline.yml`, que contiene tus archivos de activos Python, SQL y YAML

```text
claude-pipeline/
‚îú‚îÄ‚îÄ .bruin.yml                              # Entornos + conexiones (DuckDB local, BigQuery, etc.)
‚îú‚îÄ‚îÄ README.md                               # Objetivos de aprendizaje, flujo de trabajo, buenas pr√°cticas
‚îî‚îÄ‚îÄ pipeline/
    ‚îú‚îÄ‚îÄ pipeline.yml                        # Nombre del pipeline, programaci√≥n, variables
    ‚îî‚îÄ‚îÄ assets/
        ‚îú‚îÄ‚îÄ ingestion/
        ‚îÇ   ‚îú‚îÄ‚îÄ trips.py                    # Ingesti√≥n en Python
        ‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt            # Dependencias Python para la ingesti√≥n
        ‚îÇ   ‚îú‚îÄ‚îÄ payment_lookup.asset.yml    # Definici√≥n del activo seed
        ‚îÇ   ‚îî‚îÄ‚îÄ payment_lookup.csv          # Datos seed
        ‚îú‚îÄ‚îÄ staging/
        ‚îÇ   ‚îî‚îÄ‚îÄ trips.sql                   # Limpieza y transformaci√≥n
        ‚îî‚îÄ‚îÄ reports/
            ‚îî‚îÄ‚îÄ trips_report.sql            # Agregaci√≥n para anal√≠ticas
```

## üì¶ Ubicaci√≥n del proyecto

El proyecto comparte repositorio con m√°s proyectos y notas sobre el Zoomcamp de ingenier√≠a de datos de DataTalksClub. Su ubicaci√≥n respecto a la ra√≠z del proyecto Git es:

* `data-engineering-zoomcamp/05-plataformas-de-datos/pipelines/claude-pipeline`

El archivo de conexiones `.bruin.yml` se generar√° dentro de esa carpeta.

> [!WARNING]
> Bruin busca por defecto el archivo `.bruin.yml` en la ra√≠z del repositorio Git, por lo que todos los comandos deben ejecutarse desde `claude-pipeline` especificando su ruta.

``` bash
--config-file .bruin.yml
```

# Tutorial Paso a Paso

Este m√≥dulo presenta Bruin como una plataforma de datos unificada que combina **ingesti√≥n de datos**, **transformaci√≥n** y **calidad** en una √∫nica herramienta CLI. Construir√°s un pipeline de datos de extremo a extremo con los taxis de Nueva York desde cero.

> **Requisitos previos**: Familiaridad con SQL, Python b√°sico y herramientas de l√≠nea de comandos. Haber trabajado previamente con conceptos de orquestaci√≥n y transformaci√≥n es √∫til, pero no imprescindible.

---

## Parte 1: ¬øQu√© es una Plataforma de Datos?

### Objetivos de Aprendizaje
- Entender qu√© es una plataforma de datos y por qu√© la necesitas
- Aprender c√≥mo encaja Bruin en el stack de datos moderno
- Comprender las abstracciones principales de Bruin: activos, pipelines, entornos, conexiones

### 1.1 Componentes del Stack de Datos Moderno
- **Extracci√≥n/ingesti√≥n de datos**: Mover datos desde las fuentes a tu almac√©n de datos
- **Transformaci√≥n de datos**: Limpiar, modelar y agregar datos (la "T" en ELT)
- **Orquestaci√≥n de datos**: Programar y gestionar las ejecuciones del pipeline
- **Calidad/gobernanza de datos**: Garantizar la precisi√≥n y consistencia de los datos
- **Gesti√≥n de metadatos**: Rastrear linaje, propiedad y documentaci√≥n

### 1.2 D√≥nde Encaja Bruin
- Bruin = ingesti√≥n + transformaci√≥n + calidad + orquestaci√≥n en una sola herramienta
- Gestiona la orquestaci√≥n del pipeline de forma similar a Airflow (resoluci√≥n de dependencias, programaci√≥n, reintentos)
- "Qu√© pasar√≠a si Airbyte, Airflow, dbt y Great Expectations tuvieran un hijo"
- Se ejecuta localmente, en m√°quinas virtuales o en CI/CD‚Äîsin dependencia de un proveedor concreto
- C√≥digo abierto con licencia Apache

### 1.3 Principios de Dise√±o de Bruin (Conclusiones Clave)
- Todo es texto versionable (sin configuraciones en UI ni bases de datos)
- Los pipelines reales usan m√∫ltiples tecnolog√≠as (SQL + Python + R)
- Combina fuentes y destinos en un √∫nico pipeline
- La calidad de los datos es un elemento de primera clase, no algo que se a√±ade al final
- Ciclo de retroalimentaci√≥n r√°pido: CLI √°gil, desarrollo local

### 1.4 Conceptos Principales
- **Activo (Asset)**: Cualquier artefacto de datos que aporte valor (tabla, vista, archivo, modelo de ML, etc.)
- **Pipeline**: Un grupo de activos que se ejecutan juntos en orden de dependencia
- **Entorno (Environment)**: Un conjunto de configuraciones de conexi√≥n con nombre (p. ej., `default`, `production`) para que el mismo pipeline pueda ejecutarse localmente y en producci√≥n
- **Conexi√≥n (Connection)**: Credenciales para autenticarse con fuentes y destinos de datos externos
- **Ejecuci√≥n de pipeline (Pipeline run)**: Una instancia de ejecuci√≥n concreta con fechas y configuraci√≥n espec√≠ficas

---

## Parte 2: Configuraci√≥n de tu Primer Proyecto Bruin

### Objetivos de Aprendizaje
- Instalar el CLI de Bruin
- Inicializar un proyecto a partir de una plantilla
- Entender la estructura de archivos del proyecto
- Configurar entornos y conexiones

### 2.1 Instalaci√≥n
- Instala el CLI de Bruin: `curl -LsSf https://getbruin.com/install/cli | sh`
  - Verifica la instalaci√≥n: `bruin version`

Si tu terminal muestra `To use the installed binaries, please restart the shell`, haz una de las siguientes cosas:
- **Reinicia tu terminal** (cierra y vuelve a abrirla) ‚Äî la opci√≥n m√°s sencilla y fiable
- **Recarga tu shell**:
  - `exec $SHELL -l` (funciona para la mayor√≠a de shells)
  - zsh: `source ~/.zshrc`
  - bash: `source ~/.bashrc` (o `source ~/.bash_profile` en algunas configuraciones de macOS)
  - fish: `exec fish`

#### Extensi√≥n para el IDE (VS Code, Cursor, etc.)

Consulta la p√°gina de documentaci√≥n para m√°s detalles:
  - https://getbruin.com/docs/bruin/vscode-extension/overview
  - https://getbruin.com/docs/bruin/getting-started/features#vs-code-extension

1. Instala la **extensi√≥n Bruin para VS Code**:
   - Abre VS Code ‚Üí Extensiones
   - Busca: "Bruin" (editor: bruin)
   - Inst√°lala y recarga VS Code

2. Abre esta carpeta de plantilla y ejec√∫tala desde el panel de Bruin:
   - Abre `pipeline/pipeline.yml` o cualquier archivo de activo en `pipeline/assets/`
   - Usa el panel de Bruin para ejecutar `validate`, `run` y ver el c√≥digo renderizado
   - Para abrir el panel, haz clic en el logo de Bruin en la esquina superior derecha del archivo

3. Define los par√°metros de ejecuci√≥n al crear una ejecuci√≥n:
   - **Fechas de inicio y fin** para ventanas incrementales
   - **Variables personalizadas** como `taxi_types=["yellow"]`

### 2.2 Inicializaci√≥n del Proyecto
- Inicializa la plantilla zoomcamp: `bruin init zoomcamp my-pipeline`
- Explora la estructura generada:
  - `.bruin.yml` ‚Äî configuraci√≥n de entornos y conexiones
  - `pipeline/pipeline.yml` ‚Äî nombre del pipeline, programaci√≥n, variables
  - `pipeline/assets/` ‚Äî donde viven tus activos SQL/Python
  - `pipeline/assets/**/requirements.txt` ‚Äî dependencias Python (con alcance a la carpeta m√°s cercana)

**Importante**: El CLI de Bruin requiere una carpeta inicializada con git (la usa para detectar la ra√≠z del proyecto); `bruin init` inicializa git autom√°ticamente si es necesario

### 2.3 An√°lisis en Profundidad de los Archivos de Configuraci√≥n

#### `.bruin.yml`
- Define los entornos (p. ej., `default`, `production`)
- Contiene las credenciales de conexi√≥n (DuckDB, BigQuery, Snowflake, etc.)
- Se encuentra en la ra√≠z del proyecto y **debe a√±adirse al .gitignore** porque contiene credenciales y secretos
  - `bruin init` lo a√±ade autom√°ticamente al `.gitignore`, pero compru√©balo antes de hacer cualquier commit

#### `pipeline.yml`
- `name`: Identificador del pipeline (aparece en los logs y en la variable de entorno `BRUIN_PIPELINE`)
- `schedule`: Cu√°ndo ejecutarlo (`daily`, `hourly`, `weekly` o una expresi√≥n cron)
- `start_date`: Fecha m√°s antigua para los backfills
- `default_connections`: Mapeos de plataforma a conexi√≥n
- `variables`: Variables definidas por el usuario con validaci√≥n JSON Schema

### 2.4 Conexiones
- Lista las conexiones: `bruin connections list`
- A√±ade una conexi√≥n: `bruin connections add`
- Comprueba la conectividad: `bruin connections ping <connection-name>`
- Las conexiones predeterminadas reducen la repetici√≥n entre activos

---

## Parte 3: Pipeline ELT de Extremo a Extremo con Taxis de Nueva York

### Objetivos de Aprendizaje
- Construir un pipeline ELT completo: ingesti√≥n ‚Üí staging ‚Üí reports
- Entender los tres tipos de activos: Python, SQL y Seed
- Aplicar estrategias de materializaci√≥n para el procesamiento incremental
- A√±adir comprobaciones de calidad y declarar dependencias

### 3.1 Arquitectura del Pipeline
- **Ingesti√≥n**: Extrae datos en bruto de fuentes externas (activos Python, CSVs seed)
- **Staging**: Limpia, normaliza, deduplica y enriquece (activos SQL)
- **Reports**: Agrega para dashboards y anal√≠ticas (activos SQL)
- Los activos forman un DAG‚ÄîBruin los ejecuta en orden de dependencia

### 3.2 Capa de Ingesti√≥n
- Activo Python para obtener datos de taxis de Nueva York desde el endpoint p√∫blico de TLC
- Activo seed para cargar una tabla de b√∫squeda est√°tica de tipos de pago desde CSV
- Usa la estrategia `append` para la ingesti√≥n en bruto (gestiona los duplicados en capas posteriores)
- Sigue las instrucciones TODO en `pipeline/assets/ingestion/trips.py` y `pipeline/assets/ingestion/payment_lookup.asset.yml`

### 3.3 Capa de Staging
- Activo SQL para limpiar, deduplicar y unir con la tabla de b√∫squeda para enriquecer los datos de viajes en bruto
- Usa la estrategia `time_interval` para el procesamiento incremental
- Sigue las instrucciones TODO en `pipeline/assets/staging/trips.sql`

### 3.4 Capa de Reports
- Activo SQL para agregar los datos de staging en m√©tricas listas para anal√≠ticas
- Usa la estrategia `time_interval` y el mismo `incremental_key` que en staging para mantener la consistencia
- Sigue las instrucciones TODO en `pipeline/assets/reports/trips_report.sql`

### 3.5 Ejecuci√≥n y Validaci√≥n

Documentaci√≥n de comandos CLI: https://getbruin.com/docs/bruin/commands/run

```bash
# Abre una sesi√≥n shell en el directorio
cd claude-pipeline

# Valida la estructura y las definiciones
bruin validate \
    --environment default \
    --config-file .bruin.yml \
    ./pipeline/pipeline.yml

# Consejo para la primera ejecuci√≥n:
# Usa --full-refresh para crear/reemplazar tablas desde cero (√∫til con un archivo DuckDB nuevo).
bruin run \
    --environment default \
    --config-file .bruin.yml \
    --full-refresh \
    ./pipeline/pipeline.yml

# Ejecuta un activo de ingesti√≥n y luego los activos descendentes (para pruebas incrementales)
bruin run ./pipeline/assets/ingestion/trips.py \
    --environment default \
    --config-file .bruin.yml \
    --start-date 2021-01-01 \
    --end-date 2021-01-31 \
    --var taxi_types='["yellow"]' \
    --downstream

# Consulta tus tablas usando `bruin query`
# Docs: https://getbruin.com/docs/bruin/commands/query
bruin query \
    --connection duckdb-default \
    --config-file .bruin.yml \
    --query "SELECT COUNT(*) FROM ingestion.trips"

# Abre la interfaz de DuckDB (√∫til para explorar tablas de forma interactiva)
# Requiere el CLI de DuckDB instalado localmente.
duckdb duckdb.db -ui

# Comprueba el linaje para entender las dependencias entre activos
bruin lineage ./pipeline/pipeline.yml
```

---

## Parte 4: Ingenier√≠a de Datos con Agente de IA

### Objetivos de Aprendizaje
- Configurar Bruin MCP para extender los asistentes de IA con el contexto de Bruin
- Usar un agente de IA para construir el pipeline completo de extremo a extremo
- Aprovechar la IA para buscar documentaci√≥n, generar c√≥digo y ejecutar pipelines

### 4.1 ¬øQu√© es Bruin MCP?
- MCP (Model Context Protocol) conecta los asistentes de IA con las capacidades de Bruin
- La IA accede a la documentaci√≥n de Bruin, sus comandos y el contexto de tu pipeline
- Compatible con Cursor, Claude Code y otras herramientas compatibles con MCP

### 4.2 Configuraci√≥n de Bruin MCP

**Cursor IDE:**
- Ve a Configuraci√≥n de Cursor ‚Üí MCP & Integrations ‚Üí Add Custom MCP
- A√±ade la configuraci√≥n del servidor Bruin MCP:
  ```json
  {
    "mcpServers": {
      "bruin": {
        "command": "bruin",
        "args": ["mcp"]
      }
    }
  }
  ```

**Claude Code:**
```bash
claude mcp add bruin -- bruin mcp
```

Documentaci√≥n de Bruin MCP: https://getbruin.com/docs/bruin/getting-started/bruin-mcp

### 4.3 Construyendo el Pipeline con IA
- Pide a la IA que ayude a configurar `.bruin.yml` y `pipeline.yml`
- Solicita el scaffolding de activos: "Crea un activo de ingesti√≥n Python para los datos de taxis de Nueva York"
- Obt√©n ayuda con la materializaci√≥n: "¬øQu√© estrategia deber√≠a usar para cargas incrementales?"
- Depura problemas: "¬øPor qu√© falla mi comprobaci√≥n de calidad?"
- Ejecuta comandos: "Ejecuta el activo staging.trips con --full-refresh"

### 4.4 Ejemplos de Prompts

**Preguntas sobre la documentaci√≥n de Bruin:**
- "¬øC√≥mo creo una conexi√≥n DuckDB en Bruin?"
- "¬øQu√© hace la estrategia de materializaci√≥n time_interval?"
- "¬øQu√© estrategias de materializaci√≥n admite Bruin?"

**Comandos para construir o modificar el pipeline:**
- "Escribe un activo Python que obtenga datos de este endpoint de la API"
- "Genera el SQL para deduplicar viajes usando una clave compuesta"
- "A√±ade una comprobaci√≥n de calidad not_null a la columna pickup_datetime"

**Comandos para probar y validar el pipeline:**
- "Valida el pipeline completo"
- "Ejecuta el activo staging.trips con --full-refresh"
- "Comprueba el linaje de mi activo reports.trips_report"

**Comandos para consultar y analizar los datos:**
- "Ejecuta una consulta para mostrar el recuento de filas de todas mis tablas"
- "Consulta la tabla de reports para mostrar los 10 principales tipos de pago por n√∫mero de viajes"
- "Mu√©strame el esquema de datos de staging.trips"

### 4.5 Flujo de Trabajo Asistido por IA
- Empieza por la configuraci√≥n: deja que la IA ayude a configurar `.bruin.yml` y `pipeline.yml`
- Construye de forma incremental: crea un activo a la vez, valida, ejecuta e itera
- Usa la IA para la documentaci√≥n: pregunta sobre las funcionalidades de Bruin en lugar de buscar en los docs
- Depura en conjunto: comparte los mensajes de error y deja que la IA sugiera soluciones
- Aprende haciendo: haz preguntas del tipo "¬øpor qu√©?" para entender los conceptos de Bruin

Ejemplo de prompt para crear el pipeline completo de extremo a extremo y probarlo:

```text
Build an end-to-end NYC Taxi data pipeline using Bruin.

Start with running `bruin init zoomcamp` to initialize the project.

## Context
- Project folder: @zoomcamp/pipeline
- Reference docs: @zoomcamp/README.md
- Use Bruin MCP tools for documentation lookup and command execution

## Instructions

### 1. Configuration (do this first)
- Create `.bruin.yml` with a DuckDB connection named `duckdb-default`
- Configure `pipeline.yml`: set name, schedule (monthly), start_date, default_connections, and the `taxi_types` variable (array of strings)

### 2. Build Assets (follow TODOs in each file)

NYC Taxi Raw Trip Source Details:
- **URL**: `https://d37ci6vzurychx.cloudfront.net/trip-data/`
- **Format**: Parquet files, one per taxi type per month
- **Naming**: `<taxi_type>_tripdata_<year>-<month>.parquet`
- **Examples**:
  - `yellow_tripdata_2022-03.parquet`
  - `green_tripdata_2025-01.parquet`
- **Taxi Types**: `yellow` (default), `green`

Build in this order, validating each with `bruin validate` before moving on:

a) **pipeline/assets/ingestion/payment_lookup.asset.yml** - Seed asset to load CSV lookup table
b) **pipeline/assets/ingestion/trips.py** - Python asset to fetch NYC taxi parquet data from TLC endpoint
   - Use `taxi_types` variable and date range from BRUIN_START_DATE/BRUIN_END_DATE
   - Add requirements.txt with: pandas, requests, pyarrow, python-dateutil
   - Keep the data in its rawest format without any cleaning or transformations
c) **pipeline/assets/staging/trips.sql** - SQL asset to clean, deduplicate (ROW_NUMBER), and enrich with payment lookup
   - Use `time_interval` strategy with `pickup_datetime` as incremental_key
d) **pipeline/assets/reports/trips_report.sql** - SQL asset to aggregate by date, taxi_type, payment_type
   - Use `time_interval` strategy for consistency

### 3. Validate & Run
- Validate entire pipeline: `bruin validate ./pipeline/pipeline.yml`
- Run with: `bruin run ./pipeline/pipeline.yml --full-refresh --start-date 2022-01-01 --end-date 2022-02-01`
- For faster testing, use `--var 'taxi_types=["yellow"]'` (skip green taxis)

### 4. Verify Results
- Check row counts across all tables
- Query the reports table to confirm aggregations look correct
- Verify all quality checks passed (24 checks expected)
```

---

## Parte 5: Despliegue en BigQuery

Esta parte toma lo que construiste localmente y lo ejecuta en **Google BigQuery**.

> **Nota sobre dialectos SQL**: El SQL de BigQuery no es id√©ntico al de DuckDB. La estructura de tu pipeline se mantiene igual, pero puede que necesites actualizar la sintaxis SQL y los tipos de datos al cambiar de motor.

### 5.1 Crear un Proyecto GCP + Conjuntos de Datos en BigQuery
1. Crea (o selecciona) un proyecto GCP y activa la API de BigQuery
2. Crea conjuntos de datos que coincidan con los schemas de tus activos (recomendado para este m√≥dulo):
   - `ingestion`
   - `staging`
   - `reports`

### 5.2 Crear Credenciales (Elige una Opci√≥n)
- **Opci√≥n A (recomendada para desarrollo local)**: Credenciales Predeterminadas de Aplicaci√≥n (ADC)
  - Instala gcloud y autent√≠cate: `gcloud auth application-default login`
- **Opci√≥n B**: JSON de cuenta de servicio (para CI/CD)
  - Crea una cuenta de servicio con permisos de BigQuery y descarga la clave JSON

### 5.3 A√±adir la Conexi√≥n a `.bruin.yml`
```yaml
environments:
  default:
    connections:
      google_cloud_platform:
        - name: "gcp-default"
          project_id: "your-gcp-project-id"
          location: "US" # o "EU", o tu regi√≥n
          # Opciones de autenticaci√≥n (elige una):
          use_application_default_credentials: true
          # service_account_file: "/path/to/service-account.json"
          # service_account_json: |
          #   { "type": "service_account", ... }
```

### 5.4 Actualizar el Pipeline y los Activos
- En `pipeline/pipeline.yml`: cambia `default_connections.duckdb` ‚Üí `default_connections.bigquery`
  - Ejemplo: `duckdb: duckdb-default` ‚Üí `bigquery: gcp-default`
- En los activos SQL: cambia el `type` a BigQuery:
  - `duckdb.sql` ‚Üí `bq.sql`
- En los activos seed: cambia el `type` a BigQuery:
  - `duckdb.seed` ‚Üí `bq.seed`
- En los activos Python que usan materializaci√≥n: establece/actualiza `connection:` a `gcp-default`
- Corrige cualquier problema de dialecto SQL:
  - Los tipos de datos pueden diferir (p. ej., `INTEGER` vs `INT64`, manejo de timestamps, comillas)
  - Algunas funciones/operadores pueden requerir un equivalente en BigQuery

Documentaci√≥n:
- Plataforma BigQuery: https://getbruin.com/docs/bruin/platforms/bigquery
- Backend de secretos `.bruin.yml`: https://getbruin.com/docs/bruin/secrets/bruinyml

---

## Referencia de Comandos Principales

| Comando | Prop√≥sito |
|---------|-----------|
| `bruin init <template> <folder>` | Inicializa un nuevo proyecto |
| `bruin validate <path>` | Valida la estructura del pipeline/activo |
| `bruin run <path>` | Ejecuta el pipeline o el activo |
| `bruin run --downstream` | Ejecuta el activo y todos los activos descendentes |
| `bruin run --full-refresh` | Trunca y reconstruye desde cero |
| `bruin run --only checks` | Ejecuta las comprobaciones de calidad sin ejecutar el activo |
| `bruin query --connection <conn> --query "..."` | Ejecuta consultas ad-hoc |
| `bruin lineage <path>` | Visualiza las dependencias entre activos |
| `bruin render <path>` | Muestra la salida de la plantilla renderizada |
| `bruin format <path>` | Formatea el c√≥digo |
| `bruin connections list` | Lista las conexiones configuradas |
| `bruin connections ping <n>` | Comprueba la conectividad de una conexi√≥n |

---

## Buenas Pr√°cticas y Consejos

### Elegir el `incremental_key` Correcto

Al usar la estrategia `time_interval`, el `incremental_key` determina qu√© filas se eliminan y se reinsertan en cada ejecuci√≥n.

**Principios clave:**
1. **Usa la misma clave en todos los activos** ‚Äî Si staging usa `pickup_datetime` como clave incremental, reports tambi√©n deber√≠a hacerlo. Esto asegura que los datos fluyan de forma consistente a trav√©s del pipeline.

2. **Adapta la clave a tu l√≥gica de extracci√≥n de datos** ‚Äî En este ejemplo, los archivos de datos de taxis de Nueva York est√°n organizados por mes seg√∫n el inicio del viaje. Como cada archivo contiene viajes cuyo `pickup_datetime` cae en ese mes, `pickup_datetime` es la clave incremental natural.

3. **La clave debe ser inmutable** ‚Äî Una vez que se extrae una fila, el valor de su clave incremental no deber√≠a cambiar. Los timestamps de eventos (como `pickup_datetime`) son mejores que los timestamps de procesamiento por esta raz√≥n.

### Estrategia de Deduplicaci√≥n

Como los datos de taxis no tienen un ID √∫nico por fila, necesitar√°s una **clave compuesta** para la deduplicaci√≥n:

- Combina columnas que juntas identifiquen un viaje √∫nico
- Ejemplo: `(pickup_datetime, dropoff_datetime, pickup_location_id, dropoff_location_id, fare_amount)`
- Usa estas columnas con `primary_key: true` en las definiciones de tus columnas
- En SQL, deduplica usando `ROW_NUMBER()` o `QUALIFY` para conservar un registro por clave compuesta

### Desarrollo con Calidad desde el Principio

- A√±ade comprobaciones desde el inicio, no como algo que se a√±ade al final
- Usa las comprobaciones integradas: `not_null`, `unique`, `positive`, `non_negative`, `accepted_values`
- A√±ade comprobaciones personalizadas para invariantes espec√≠ficos del negocio

### Organizaci√≥n del Proyecto

- Mant√©n los activos en `pipeline/assets/`
- Usa schemas para organizar las capas: `ingestion.`, `staging.`, `reports.`
- Coloca el SQL que no sea de activos en carpetas separadas (`/analyses`, `/queries`)

### Desarrollo Local

- Valida siempre antes de ejecutar: `bruin validate ./pipeline/pipeline.yml`
- Usa `--full-refresh` para las primeras ejecuciones en bases de datos nuevas
- Consulta las tablas directamente para verificar los resultados: `bruin query --connection duckdb-default --query "..."`
- Comprueba el linaje para entender el impacto de los cambios: `bruin lineage <asset>`